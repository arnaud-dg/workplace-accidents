{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deec906b",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center;font-size:200%;\">\n",
    "    <b>Workplace Accident Database Textual Analysis through LLMs</b>\n",
    "</h2>\n",
    "<h3  style=\"text-align:center;\">Keywords : \n",
    "    <span style=\"border-radius:7px;background-color:yellowgreen;color:white;padding:7px;\">Large Language Models</span>\n",
    "    <span style=\"border-radius:7px;background-color:yellowgreen;color:white;padding:7px;\">Natural Language Processing</span>\n",
    "    <span style=\"border-radius:7px;background-color:yellowgreen;color:white;padding:7px;\">Mistral</span>\n",
    "    <span style=\"border-radius:7px;background-color:yellowgreen;color:white;padding:7px;\">Work Accidents</span>\n",
    "    <span style=\"border-radius:7px;background-color:yellowgreen;color:white;padding:7px;\">EHS</span>\n",
    "</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362650f5",
   "metadata": {},
   "source": [
    "The [EPICEA database](https://www.inrs.fr/publications/bdd/epicea.html) is managed by a french institute called [INRS](https://www.inrs.fr/), in charge of risk preventions in work environments.\n",
    "\n",
    "The purpose of this notebook is to create a tool able to : \n",
    "1. extract massively the accident descriptions from the french \"EPICEA\" database.,\n",
    "2. apply a LLM prompt in order to extract structured information from unstructured description\n",
    "3. propose a first analysis of the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b94c053",
   "metadata": {},
   "source": [
    "Epicea est une base de données nationale et anonyme rassemblant plus de 21 000 cas d'accidents du travail survenus, depuis 1990, à des salariés du régime général de la Sécurité sociale. Ces accidents sont mortels, graves ou significatifs pour la prévention.\n",
    "\n",
    "Cette base de données n'est pas exhaustive puisque tous les accidents du travail n'y sont pas répertoriés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5236dd7",
   "metadata": {},
   "source": [
    "L'anonymat des personnes physiques et morales est respecté et l'origine des informations est préservée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de56df4",
   "metadata": {},
   "source": [
    "Le numéro du dossier (qui s'incrémente automatiquement) : plus le numéro est élevé, plus l'accident est récent\n",
    "Le comité technique national (classification des grands secteurs d'activité selon l'arrêté du 17 octobre 1995 modifié)\n",
    "Le code entreprise (jusqu'en 2015 : code risque, déclinaison des comités techniques nationaux ; à partir de janvier 2015 : code APE selon la nomenclature NAF)\n",
    "Le facteur matériel le plus proche des lésions : objet, matériel, matériau, installation, etc. intervenant dans l'accident\n",
    "Le récit circonstancié de l'accident, éventuellement complété par des documents attachés (photos, arbres des causes, schémas, etc.)\n",
    "\n",
    "Le facteur matériel (ou matériel en cause) est structuré et renvoie à un libellé plus ou moins détaillé. Par exemple 510210 concerne les toitures en matériaux fragiles, 5102* une partie de bâtiment ou d’ouvrage, 51* les zones géographiques et emplacements de travail.\n",
    "\n",
    "Une collection de dossiers est obtenue par sélection multicritère."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf762dbf",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: left; background-color: yellowgreen; color: white; padding: 10px; line-height:1;border-radius:10px\">1. Modules and dependancies installing</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d65579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System path configuration (if necessary)\n",
    "sys.path.append(\"C:/Users/arnaud/AppData/Roaming/Python/Python312/site-packages/\")\n",
    "sys.path.append(\"C:/Windows/System32/\")\n",
    "sys.path.append(\"C:/Users/Arnaud/AppData/Roaming/Python/Python312/site-packages/onnxruntime/capi/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5e23bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from ast import literal_eval\n",
    "from enum import Enum\n",
    "from io import StringIO\n",
    "from os.path import exists\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Sequence, Generic, TypeVar\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Data and analysis libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Natural language processing and AI libraries\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from ollama import Client\n",
    "import openai\n",
    "\n",
    "# Web scraping and automation libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Visualization and user interface libraries\n",
    "from IPython.display import Markdown as md\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data validation and modeling libraries\n",
    "from pydantic import BaseModel, Field, Extra, validator, ConfigDict, field_validator\n",
    "\n",
    "# Ollama specific imports\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "531ef56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of helpful functions located in helper.py\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51fed825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Chrome options for headless mode\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "# Initialize the Chrome driver with the specified options\n",
    "driver = webdriver.Chrome() \n",
    "waiting_time = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096d5528",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: left; background-color: yellowgreen; color: white; padding: 10px; line-height:1;border-radius:10px\">2. Data Collection from INRS website</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065b7228",
   "metadata": {},
   "source": [
    "Web scraping, is a technique used in data science to automatically extract data from websites.\n",
    "It involves using a program or script to navigate through web pages, parse the HTML or XML code, and extract specific pieces of information, such as text, images, files or other structured data. \n",
    "\n",
    "Our web scraping strategy will be performed in 2 separate steps:\n",
    "- First we will get the list of accident #IDs available in the database\n",
    "- Then we will extract separately the informations related to each individual accident."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e221fe57",
   "metadata": {},
   "source": [
    "## 2.1. Accidents #ID extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "985baa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_accident_ids():\n",
    "    \"\"\"Main function to extract all accident IDs.\"\"\"\n",
    "    driver = initialize_driver()\n",
    "    try:\n",
    "        navigate_to_search_page(driver)\n",
    "        perform_search(driver)\n",
    "        display_list(driver)\n",
    "        total_pages = get_total_pages(driver)\n",
    "        accident_ids = extract_accident_ids(driver, total_pages)\n",
    "        total_ids = save_accident_ids(accident_ids)\n",
    "        print(f\"Extraction complete. Total accident IDs: {total_ids}\")\n",
    "        return total_ids\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1692cf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▎                                                                          | 132/4349 [03:36<1:55:17,  1.64s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Execute the extraction\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m total_ids \u001b[38;5;241m=\u001b[39m extract_all_accident_ids()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal number of accident IDs extracted: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_ids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m, in \u001b[0;36mextract_all_accident_ids\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m display_list(driver)\n\u001b[0;32m      8\u001b[0m total_pages \u001b[38;5;241m=\u001b[39m get_total_pages(driver)\n\u001b[1;32m----> 9\u001b[0m accident_ids \u001b[38;5;241m=\u001b[39m extract_accident_ids(driver, total_pages)\n\u001b[0;32m     10\u001b[0m total_ids \u001b[38;5;241m=\u001b[39m save_accident_ids(accident_ids)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtraction complete. Total accident IDs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_ids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\workplace-accidents\\helper.py:65\u001b[0m, in \u001b[0;36mextract_accident_ids\u001b[1;34m(driver, total_pages)\u001b[0m\n\u001b[0;32m     63\u001b[0m         next_page_button \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mLINK_TEXT, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     64\u001b[0m         next_page_button\u001b[38;5;241m.\u001b[39mclick()\n\u001b[1;32m---> 65\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1.5\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(accident_ids))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Execute the extraction\n",
    "total_ids = extract_all_accident_ids()\n",
    "print(f\"Total number of accident IDs extracted: {total_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a51a58",
   "metadata": {},
   "source": [
    "## 2.2. Detailed data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bda365",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_analyzed = load_data()\n",
    "df = filter_unanalyzed_data(df, df_analyzed)\n",
    "df = initialize_dataframe(df)\n",
    "new_data = process_accidents(df)\n",
    "\n",
    "# Combine new data with existing analyzed data\n",
    "df_analyzed = pd.concat([df_analyzed, new_data], ignore_index=True)\n",
    "df_analyzed.to_csv('Accident_database.csv', sep='|', index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Data extraction complete. Total accidents in database: {len(df_analyzed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bce764",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: left; background-color: yellowgreen; color: white; padding: 10px; line-height:1;border-radius:10px\">3. Extraction of data from narratives</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8e4405",
   "metadata": {},
   "source": [
    "A part of the code will use prompt and variable name formulated in french. Because the data source in written in french, it is necessary, for better results, to write the prompts in french and to describe the expected output in french."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231581f5",
   "metadata": {},
   "source": [
    "## 3.1. Classes description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05bb4cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BodyZone(str, Enum):\n",
    "    HEAD = \"tete\"\n",
    "    CHEST = \"torse\"\n",
    "    STOMACH = \"ventre\"\n",
    "    BACK = \"dos\"\n",
    "    ARM = \"bras\"\n",
    "    HAND = \"main\"\n",
    "    LEG = \"jambe\"\n",
    "    FOOT = \"pied\"\n",
    "    POSTERIOR = \"posterieur\"\n",
    "    HEART = \"coeur\"\n",
    "    NA = \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd1c7b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arnaud\\AppData\\Local\\Temp\\ipykernel_38960\\1841967672.py:18: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/\n",
      "  @validator('Sexe')\n",
      "C:\\Users\\Arnaud\\AppData\\Local\\Temp\\ipykernel_38960\\1841967672.py:24: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/\n",
      "  @validator('Age')\n",
      "C:\\Users\\Arnaud\\AppData\\Local\\Temp\\ipykernel_38960\\1841967672.py:30: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/\n",
      "  @validator('Metier', 'Type_accident', 'Blessure')\n",
      "C:\\Users\\Arnaud\\AppData\\Local\\Temp\\ipykernel_38960\\1841967672.py:36: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/\n",
      "  @validator('Machine', 'Cause')\n",
      "C:\\Users\\Arnaud\\AppData\\Local\\Temp\\ipykernel_38960\\1841967672.py:42: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/\n",
      "  @validator('Zone')\n"
     ]
    }
   ],
   "source": [
    "class Accident(BaseModel):\n",
    "    Metier: str = Field(description=\"Victim's job, role or function who suffered the accident.\")\n",
    "    Sexe: str = Field(description=\"Sex (Man or Woman) of the victim who suffered the accident.\")\n",
    "    Age: int = Field(description=\"Age of the victim who suffered the accident.\")\n",
    "    \n",
    "    Type_accident: str = Field(description=\"Type of accident that occurred. 1 or 2 words maximum.\")\n",
    "    Blessure: str = Field(description=\"Medical description of injuries or symptoms. 1 or 2 words maximum.\")\n",
    "    \n",
    "    Deces: bool = Field(description=\"The victim is mentioned as deceased.\")\n",
    "    Circulation: bool = Field(description=\"Accident related to traffic.\")\n",
    "    Malaise: bool = Field(description=\"Accident related to a medical condition such as stroke, heart attack.\")\n",
    "    Suicide: bool = Field(description=\"Accident related to suicide.\")\n",
    "    \n",
    "    Machine: List[str] = Field(description=\"Machines, parts or objects involved in the accident. 1 or 2 words maximum per item.\")\n",
    "    Cause: List[str] = Field(description=\"Factors that directly caused or contributed to the accident. 1 to 3 words maximum per factor.\")\n",
    "    Zone: BodyZone = Field(description=\"Body area affected by the accident.\")\n",
    "        \n",
    "    @field_validator('Sexe')\n",
    "    @classmethod\n",
    "    def sexe_valide(cls, v):\n",
    "        if v.lower() not in ['homme', 'femme']:\n",
    "            raise ValueError('Sex must be \"Homme\" or \"Femme\"')\n",
    "        return v.capitalize()\n",
    "\n",
    "    @field_validator('Age')\n",
    "    @classmethod\n",
    "    def age_valide(cls, v):\n",
    "        if v is not None and (v < -1 or v > 120):\n",
    "            raise ValueError('Age must be between 0 and 120')\n",
    "        return v\n",
    "\n",
    "    @field_validator('Metier', 'Type_accident', 'Blessure')\n",
    "    @classmethod\n",
    "    def non_vide(cls, v):\n",
    "        if not v.strip():\n",
    "            raise ValueError('This field cannot be empty')\n",
    "        return v\n",
    "\n",
    "    @field_validator('Machine', 'Cause')\n",
    "    @classmethod\n",
    "    def liste_non_vide(cls, v):\n",
    "        if not v:\n",
    "            return ['Not specified']\n",
    "        return [item.strip() for item in v if item.strip()]\n",
    "\n",
    "    @field_validator('Zone')\n",
    "    @classmethod\n",
    "    def zone_valide(cls, v):\n",
    "        zone_mapping = {\n",
    "            'tete': ['crane', 'visage', 'cou', 'cerveau'],\n",
    "            'torse': ['poitrine', 'torse', 'poumon'],\n",
    "            'ventre': ['ventre', 'estomac'],\n",
    "            'dos': ['dos', 'epaule'],\n",
    "            'bras': ['bras', 'coude', 'epaule'],\n",
    "            'main': ['main', 'doigt', 'poignet'],\n",
    "            'jambe': ['genou', 'cuisse', 'mollet', 'tibia'],\n",
    "            'pied': ['pied', 'cheville'],\n",
    "            'posterieur': ['fesses'],\n",
    "            'coeur': ['coeur']\n",
    "        }\n",
    "        \n",
    "        v = v.lower()\n",
    "        for zone, keywords in zone_mapping.items():\n",
    "            if v in keywords:\n",
    "                return BodyZone(zone)\n",
    "        return BodyZone.NA\n",
    "\n",
    "    model_config = ConfigDict(\n",
    "        extra='forbid',\n",
    "        use_enum_values=True,\n",
    "        json_schema_extra={\n",
    "            'examples': [\n",
    "                {\n",
    "                    'Metier': 'Maintenance technician',\n",
    "                    'Sexe': 'Homme',\n",
    "                    'Age': 45,\n",
    "                    'Type_accident': 'Fall',\n",
    "                    'Blessure': 'Fracture',\n",
    "                    'Deces': False,\n",
    "                    'Circulation': False,\n",
    "                    'Malaise': False,\n",
    "                    'Suicide': False,\n",
    "                    'Machine': ['Ladder'],\n",
    "                    'Cause': ['Slippery floor', 'Lack of PPE'],\n",
    "                    'Zone': 'jambe'\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898cd944",
   "metadata": {},
   "source": [
    "## 3.2. Runinng functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b42325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_llm_and_prompt():\n",
    "    \"\"\"Set up the language model and prompt for accident analysis.\"\"\"\n",
    "    pydantic_parser = PydanticOutputParser(pydantic_object=Accident)\n",
    "    format_instructions = pydantic_parser.get_format_instructions()\n",
    "\n",
    "    template_string = \"\"\"You are a French analyst reviewing accident reports and performing data entry. \n",
    "    Analyze the text below between triple apostrophes and extract the required information. \n",
    "\n",
    "    Accident description: ```{descriptif}```\n",
    "\n",
    "    IMPORTANT:\n",
    "    - All your answers MUST be in French.\n",
    "    - For the 'Sexe' field, use ONLY 'Homme' or 'Femme'.\n",
    "    - The 'Metier' field must be a string, not a list.\n",
    "    - For the 'Zone' field, use ONLY one of the following values according to the affected body area:\n",
    "      - 'tete' for [crane, visage, cou, cerveau]\n",
    "      - 'torse' for [poitrine, torse, poumon]\n",
    "      - 'ventre' for [ventre, estomac]\n",
    "      - 'dos' for [dos, epaule]\n",
    "      - 'bras' for [bras, coude, epaule]\n",
    "      - 'main' for [main, doigt, poignet]\n",
    "      - 'jambe' for [genou, cuisse, mollet, tibia]\n",
    "      - 'pied' for [pied, cheville]\n",
    "      - 'posterieur' for [fesses]\n",
    "      - 'coeur' for [coeur]\n",
    "      - 'NA' if the information is not present\n",
    "    - If the information does not appear in the narrative, use 'NA' for text fields, \n",
    "\n",
    "    Your response MUST be a valid JSON object, strictly adhering to the following schema. Do NOT include ANY text outside this JSON object.\n",
    "\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate(\n",
    "        messages=[\n",
    "            HumanMessagePromptTemplate.from_template(template_string)  \n",
    "        ],\n",
    "        input_variables=[\"descriptif\"],\n",
    "        partial_variables={\"format_instructions\": format_instructions}\n",
    "    )\n",
    "\n",
    "    llm = ChatOllama(\n",
    "        model=\"mistral\", \n",
    "        format=\"json\",\n",
    "        temperature=0,\n",
    "        top_k=10,\n",
    "        top_p=0.9,\n",
    "        repeat_penalty=1.1\n",
    "    )\n",
    "\n",
    "    return llm, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8170eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(csv_name=\"Accident_database_refined.csv\"):\n",
    "    \"\"\"Load and prepare the accident data for analysis.\"\"\"\n",
    "    if Path(csv_name).is_file():\n",
    "        df = pd.read_csv(csv_name, sep=\"|\")\n",
    "    else:\n",
    "        df = pd.read_csv('Accident_database.csv', sep=\"|\")\n",
    "        new_columns = ['Metier', 'Sexe', 'Age', 'Type_accident', 'Blessure', 'Deces', 'Circulation', 'Malaise',\n",
    "                       'Suicide', 'Machine', 'Cause', 'Zone', 'Status']\n",
    "        for col in new_columns:\n",
    "            df[col] = None\n",
    "        \n",
    "        # Filter data based on specific enterprise codes\n",
    "        enterprise_codes = ['241GM', '274CG', '295EC', '2110Z', '2120Z', '244CB', '244DA', '4646Z', '4773Z', '514NA', \n",
    "                            '523AB', '1073Z', '1086Z', '1089Z', '157AB', '158VB']\n",
    "        df = df[df['Code_entreprise'].apply(lambda x: any(code in x for code in enterprise_codes))]\n",
    "        df = df[df['Numero_dossier'] != 19258]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fe4862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_accidents(df, llm, prompt):\n",
    "    \"\"\"Analyze accidents using the LLM and update the dataframe.\"\"\"\n",
    "    unanalyzed_refs = df.loc[df['Status'].isnull(), 'Numero_dossier'].tolist()\n",
    "\n",
    "    for num in tqdm(unanalyzed_refs):\n",
    "        descriptif = df.loc[df['Numero_dossier'] == num, 'Resume'].item()\n",
    "        messages = prompt.format_messages(descriptif=descriptif)\n",
    "\n",
    "        chat_model_response = llm.invoke(messages)\n",
    "        print(f\"Raw response for number {num}:\")\n",
    "        print(datetime.datetime.now())\n",
    "        print(chat_model_response.content)\n",
    "\n",
    "        content_dict = parse_json_safely(chat_model_response.content)\n",
    "\n",
    "        if content_dict:\n",
    "            content_dict = process_content_dict(content_dict)\n",
    "            content_dict = add_default_values(content_dict)\n",
    "            content_dict = clean_and_standardize_content(content_dict)\n",
    "\n",
    "            if validate_content(content_dict):\n",
    "                for key in content_dict.keys():\n",
    "                    df.loc[df['Numero_dossier'] == num, key] = content_dict[key]\n",
    "                df.loc[df['Numero_dossier'] == num, 'Status'] = 'Analyzed'\n",
    "            else:\n",
    "                print(f\"Warning: Incomplete result for number {num}\")\n",
    "                print(f\"Dictionary content: {content_dict}\")\n",
    "        else:\n",
    "            print(f\"Error: Unable to parse the result for number {num}\")\n",
    "            print(f\"Description for number {num}:\")\n",
    "            print(descriptif)\n",
    "            continue\n",
    "\n",
    "        # Incremental save\n",
    "        df.to_csv('Accident_database_refined.csv', sep='|', index=False, encoding=\"utf-8\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e357db9",
   "metadata": {},
   "source": [
    "## 3.3. Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "llm, prompt = setup_llm_and_prompt()\n",
    "df = load_and_prepare_data()\n",
    "df = analyze_accidents(df, llm, prompt)\n",
    "\n",
    "inference_time = time.time() - start_time\n",
    "print(\"Analysis completed.\")\n",
    "print(f\"Total inference time: {inference_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
